import os
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
import json
import uvicorn


# Load environment variables from .env file
load_dotenv()

def initialize_llm() -> ChatOpenAI:
    """Initialize and return a ChatOpenAI model instance for streaming.
    
    Supports multiple LLM providers via environment variables.
    
    Streaming allows responses to be sent in chunks as they're generated:
    - Real-time feedback: Users see responses as they're generated
    - Better UX: Reduces perceived latency
    - Progressive rendering: Display content incrementally
    """
    provider = os.getenv("LLM_PROVIDER", "openai").lower()
    api_key = os.getenv(f"{provider.upper()}_API_KEY")
    model_name = os.getenv(f"{provider.upper()}_MODEL_NAME")
    base_url = os.getenv(f"{provider.upper()}_BASE_URL")
    
    if not api_key:
        raise ValueError(f"{provider.upper()}_API_KEY environment variable is required.")
    
    if not model_name:
        raise ValueError(f"{provider.upper()}_MODEL_NAME environment variable is required.")
    
    # Generic ChatOpenAI initialization
    config = {
        "model": model_name,
        "api_key": api_key,
        "base_url": base_url,
        "max_retries": 2,
        "streaming": True,  # Enable streaming
    }
    
    return ChatOpenAI(**config)

# Create FastAPI app
provider = os.getenv("LLM_PROVIDER", "openai").lower()
app = FastAPI(
    title="LangChain + Multi-Provider LLM with Streaming",
    version="2.0.0",
    description=f"Real-time streaming responses from LLM. Currently using {provider.upper()} provider"
)


class ChatRequest(BaseModel):
    message: str


class ChatResponse(BaseModel):
    response: str
    model: str
    provider: str


async def generate_stream(prompt: str):
    """Generate streaming response chunks.
    
    Yields chunks of data as they're generated by the LLM.
    Each chunk is sent as a JSON object followed by a newline.
    """
    try:
        llm = initialize_llm()
        
        # Stream chunks from the LLM
        for chunk in llm.stream(prompt):
            if hasattr(chunk, "content") and chunk.content:
                # Send each chunk as JSON
                data = {
                    "chunk": chunk.content,
                    "done": False
                }
                yield f"data: {json.dumps(data)}\n\n"
        
        # Send completion signal
        data = {"chunk": "", "done": True}
        yield f"data: {json.dumps(data)}\n\n"
        
    except Exception as e:
        error_data = {"error": str(e), "done": True}
        yield f"data: {json.dumps(error_data)}\n\n"


@app.post("/chat")
def chat(request: ChatRequest) -> ChatResponse:
    """
    Standard non-streaming chat endpoint for comparison.
    
    Workflow:
    1. Environment Setup: Load API key and model configuration from .env
    2. Instantiation: ChatOpenAI created with streaming disabled
    3. Invocation: Call .invoke() method (blocks until complete)
    4. Response: Return complete response at once
    """
    try:
        llm = initialize_llm()
        prompt = f"You are a helpful assistant. Please respond to the user's message.\n\nUser: {request.message}\n"
        
        result = llm.invoke(prompt)
        content = result.content if hasattr(result, "content") else str(result)
        
        # Get current provider and model name
        current_provider = os.getenv("LLM_PROVIDER", "openai").lower()
        if current_provider == "openai":
            model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini")
        else:
            model_name = os.getenv("GEMINI_MODEL_NAME", "unknown")
        
        return ChatResponse(
            response=content,
            model=model_name,
            provider=current_provider
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Streaming chat endpoint demonstrating real-time response generation.
    
    Workflow:
    1. Environment Setup: Load API key and model configuration from .env
    2. Instantiation: ChatOpenAI created with streaming=True
    3. Stream Invocation: Call .stream() method (returns iterator)
    4. Yield Chunks: Send each chunk as it's generated
    5. Completion: Signal when streaming is complete
    
    Benefits:
    - Real-time feedback: Users see responses as they're generated
    - Better UX: Reduces perceived latency for long responses
    - Progressive rendering: Display content incrementally
    - Responsive: Users know the system is working
    
    Returns:
        StreamingResponse with Server-Sent Events (SSE) format
    """
    try:
        prompt = f"You are a helpful assistant. Please respond to the user's message.\n\nUser: {request.message}\n"
        
        return StreamingResponse(
            generate_stream(prompt),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # Disable buffering in nginx
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")


@app.get("/")
def root():
    """Root endpoint with API information."""
    current_provider = os.getenv("LLM_PROVIDER", "openai").lower()
    if current_provider == "openai":
        model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini")
    else:
        model_name = os.getenv("GEMINI_MODEL_NAME", "unknown")
    
    return {
        "message": "LangChain Streaming Demo",
        "provider": current_provider,
        "model": model_name,
        "endpoints": {
            "chat": "/chat (POST) - Non-streaming response",
            "stream": "/chat/stream (POST) - Streaming response"
        }
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
