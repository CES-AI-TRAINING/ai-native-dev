"""
Demo 07: LangChain MCP Integration

Integrate MCP tools with LangChain agents:
- Convert FastMCP tools to LangChain tools
- Build agents that use MCP servers
- Framework interoperability
- Multi-tool orchestration

Key Concepts:
- FastMCP + LangChain integration
- Tool wrapping patterns
- Agent-based workflows
- Cross-framework compatibility
"""

import asyncio
import os
import sys
from typing import Any, Optional, Type
from dotenv import load_dotenv

# FastMCP import (always needed)
from fastmcp import FastMCP

# Check if running in server mode FIRST (before any LangChain imports)
is_server_mode = len(sys.argv) > 1 and sys.argv[1] == "--server"

# LangChain imports (only when NOT in server mode)
if not is_server_mode:
    try:
        from langchain.tools import BaseTool
        from langchain_openai import ChatOpenAI
        from langchain.agents import AgentExecutor, create_openai_functions_agent
        from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
        from pydantic import BaseModel
    except ImportError as e:
        print(f"âš ï¸  LangChain import error: {e}")
        print("    Install with: uv sync")
        print("    Or run in server mode: python main.py --server")
        raise
else:
    # In server mode, define dummy classes so module can load
    # These won't be used, but Python needs them for parsing
    class BaseTool:
        pass
    class ChatOpenAI:
        pass
    class AgentExecutor:
        pass
    def create_openai_functions_agent(*args, **kwargs):
        pass
    class ChatPromptTemplate:
        pass
    class MessagesPlaceholder:
        pass
    class BaseModel:
        pass

# ============================================================================
# CONFIGURATION
# ============================================================================

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# Only check API key when NOT in server mode
if not is_server_mode:
    if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
        print("âš ï¸  WARNING: OPENAI_API_KEY not configured")
        print("   This demo requires an OpenAI API key.")
        print("   1. Get key from https://platform.openai.com/")
        print("   2. Copy .env.example to .env")
        print("   3. Add your key to .env")
        print()
        exit(1)

    print("=" * 70)
    print("MCP DEMO 07: LANGCHAIN INTEGRATION")
    print("=" * 70)
    print(f"âœ“ OpenAI Model: {OPENAI_MODEL}")
    print()

# ============================================================================
# FASTMCP SERVER (Calculator tools)
# ============================================================================

mcp = FastMCP("Calculator Server")


@mcp.tool()
def add(a: float, b: float) -> dict:
    """Add two numbers together."""
    result = a + b
    return {"result": result, "expression": f"{a} + {b} = {result}"}


@mcp.tool()
def subtract(a: float, b: float) -> dict:
    """Subtract b from a."""
    result = a - b
    return {"result": result, "expression": f"{a} - {b} = {result}"}


@mcp.tool()
def multiply(a: float, b: float) -> dict:
    """Multiply two numbers."""
    result = a * b
    return {"result": result, "expression": f"{a} Ã— {b} = {result}"}


@mcp.tool()
def divide(a: float, b: float) -> dict:
    """Divide a by b."""
    if b == 0:
        return {"error": "Cannot divide by zero"}
    result = a / b
    return {"result": result, "expression": f"{a} Ã· {b} = {result}"}


# ============================================================================
# FASTMCP TO LANGCHAIN ADAPTER
# ============================================================================

class MCPTool(BaseTool):
    """
    LangChain tool wrapper for FastMCP tools.
    
    This adapter allows FastMCP tools to be used as LangChain tools,
    enabling integration with LangChain agents and chains.
    """
    
    name: str
    description: str
    mcp_tool_func: Any
    
    def __init__(self, mcp_tool_func: Any):
        """
        Initialize MCPTool from FastMCP tool function.
        
        Args:
            mcp_tool_func: The FastMCP tool function
        """
        super().__init__(
            name=mcp_tool_func.__name__,
            description=mcp_tool_func.__doc__ or f"FastMCP tool: {mcp_tool_func.__name__}",
            mcp_tool_func=mcp_tool_func
        )
    
    def _run(self, **kwargs) -> str:
        """
        Synchronous execution (required by LangChain).
        """
        try:
            result = self.mcp_tool_func(**kwargs)
            # Extract result from dict if present
            if isinstance(result, dict):
                if "error" in result:
                    return f"Error: {result['error']}"
                if "result" in result:
                    return str(result["result"])
            return str(result)
        except Exception as e:
            return f"Error: {str(e)}"
    
    async def _arun(self, **kwargs) -> str:
        """Asynchronous execution."""
        return self._run(**kwargs)


def create_langchain_tools_from_fastmcp(mcp_instance: FastMCP) -> list[MCPTool]:
    """
    Convert all FastMCP tools to LangChain tools.
    
    Args:
        mcp_instance: FastMCP instance
        
    Returns:
        List of LangChain-compatible tools
    """
    langchain_tools = []
    
    # Access FastMCP's tool registry
    if hasattr(mcp_instance, '_tools'):
        for tool_func in mcp_instance._tools:
            lc_tool = MCPTool(mcp_tool_func=tool_func)
            langchain_tools.append(lc_tool)
    
    return langchain_tools


# ============================================================================
# LANGCHAIN AGENT
# ============================================================================

def create_agent_with_mcp_tools() -> AgentExecutor:
    """
    Create a LangChain agent with FastMCP tools.
    
    Returns:
        Configured agent executor
    """
    # Convert FastMCP tools to LangChain tools
    tools = create_langchain_tools_from_fastmcp(mcp)
    
    print(f"âœ“ Loaded {len(tools)} FastMCP tools into LangChain")
    for tool in tools:
        print(f"  â€¢ {tool.name}: {tool.description}")
    print()
    
    # Initialize LLM
    llm = ChatOpenAI(
        model=OPENAI_MODEL,
        temperature=0,
        api_key=OPENAI_API_KEY
    )
    
    # Create prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful assistant with access to calculator tools.
        
When asked to perform calculations:
1. Break down complex calculations into steps
2. Use the appropriate tools (add, subtract, multiply, divide)
3. Show your work
4. Provide the final answer clearly

Available tools:
- add(a, b): Add two numbers
- subtract(a, b): Subtract b from a
- multiply(a, b): Multiply two numbers
- divide(a, b): Divide a by b
"""),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    # Create agent
    agent = create_openai_functions_agent(llm, tools, prompt)
    
    # Create executor
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors=True
    )
    
    return agent_executor


# ============================================================================
# DEMOS
# ============================================================================

async def demo_langchain_mcp():
    """Demonstrate LangChain agent using FastMCP tools."""
    
    print("=" * 70)
    print("LANGCHAIN + FASTMCP: Agent Demonstrations")
    print("=" * 70)
    print()
    
    # Create agent
    agent = create_agent_with_mcp_tools()
    
    # Test cases
    test_queries = [
        "What is 15 plus 27?",
        "Calculate (100 - 25) multiplied by 3", 
        "What is 144 divided by 12, then add 50?",
        "Solve: ((15 + 25) * 2) / 4"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"{'=' * 70}")
        print(f"Query {i}: {query}")
        print(f"{'=' * 70}")
        print()
        
        try:
            result = agent.invoke({"input": query})
            print()
            print(f"âœ“ Final Answer: {result['output']}")
            print()
        except Exception as e:
            print(f"âœ— Error: {e}")
            print()
    
    print("=" * 70)


async def main():
    """Run the LangChain+FastMCP integration demo."""
    
    print()
    print("â•”" + "=" * 68 + "â•—")
    print("â•‘" + " " * 15 + "WELCOME TO MCP DEMO 07" + " " * 31 + "â•‘")
    print("â•‘" + " " * 14 + "LangChain MCP Integration" + " " * 29 + "â•‘")
    print("â•š" + "=" * 68 + "â•")
    print()
    
    print("ðŸ“š This demo demonstrates:")
    print("   â€¢ Converting FastMCP tools to LangChain tools")
    print("   â€¢ Building agents with MCP tool access")
    print("   â€¢ Framework interoperability")
    print("   â€¢ Multi-tool orchestration by LLM")
    print("   â€¢ Real-world agent workflows")
    print()
    
    # Run demo
    await demo_langchain_mcp()
    
    print("=" * 70)
    print("ðŸ’¡ KEY TAKEAWAYS")
    print("=" * 70)
    print()
    print("âœ“ FastMCP tools can be used in any framework via adapters")
    print("âœ“ LangChain agents can orchestrate multiple FastMCP tools")
    print("âœ“ Write tools once (FastMCP), use everywhere (LangChain, etc.)")
    print("âœ“ Standardization reduces integration complexity")
    print("âœ“ Agents decide which tools to use and in what order")
    print()
    
    print("=" * 70)
    print("ðŸ” HOW IT WORKS")
    print("=" * 70)
    print()
    print("1. FastMCP Server: Provides calculator tools")
    print("2. MCPTool Wrapper: Converts to LangChain format")
    print("3. LangChain Agent: Orchestrates tool usage")
    print("4. OpenAI LLM: Decides which tools to call")
    print("5. Tools Execute: FastMCP functions run")
    print("6. Results: Agent synthesizes final answer")
    print()
    
    print("=" * 70)
    print("ðŸŽ¯ NEXT STEPS")
    print("=" * 70)
    print()
    print("1. Add more FastMCP servers:")
    print("   â€¢ Weather tools for real-time data")
    print("   â€¢ Database tools for data access")
    print("   â€¢ Filesystem tools for file operations")
    print()
    
    print("2. Explore advanced patterns:")
    print("   â€¢ Multi-server coordination")
    print("   â€¢ LangGraph for complex workflows")
    print("   â€¢ Streaming responses")
    print()
    
    print("ðŸ’¡ Exercise: Integrate Demo 03 weather tools")
    print("   Add weather.py server and use both calculators + weather")


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--server":
        # Run as MCP server only (for testing MCP protocol)
        print("âœ“ LangChain MCP server starting...", file=sys.stderr)
        print("âœ“ Server: Calculator Server", file=sys.stderr)
        print("âœ“ Transport: stdio", file=sys.stderr)
        print("âœ“ Tools: 4 calculator operations", file=sys.stderr)
        print("âœ“ Ready for client connections", file=sys.stderr)
        print("âœ“ Note: MCP tools only (not LangChain agent)", file=sys.stderr)
        print(file=sys.stderr)
        mcp.run()
    else:
        # Run LangChain demo (requires OPENAI_API_KEY)
        asyncio.run(main())
